{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c675a267-9b43-474a-9bbc-441b42c2da97",
   "metadata": {},
   "source": [
    "Question Answering with ü§ó\n",
    "==============================\n",
    "\n",
    "\n",
    "In this second task, we're going to stick to the ü§ó transformers library, but this time we're going to train a **generative question answering** model. We'll also stick to the **SQuAD** dataset.\n",
    "\n",
    "## Setup\n",
    "Let's double check that we have access to a GPU unit üôè\n",
    "\n",
    "If the following command errors, then you should change the environment in google colab settings to one with GPU. \n",
    "The correct output should look something like the following:\n",
    "\n",
    "```\n",
    "Thu Feb  2 20:34:38 2023       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla V100-SXM2...  On   | 00000000:5B:00.0 Off |                    0 |\n",
    "| N/A   37C    P0    40W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a944bf3d-bec4-4a8e-90b4-c1cadb9ea1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a68c36-85c3-4100-b09b-bff66f7dfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should not need to set this if you're using google colab:\n",
    "# %set_env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45a29e-72a1-4ab2-854a-1ef853abd5e5",
   "metadata": {
    "id": "kJ_GoQx_K6sC"
   },
   "source": [
    "Now let's go ahead and install all needed dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2ff61-4ea9-4c6d-9222-8e695f723713",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeJtMsvBJ48f",
    "outputId": "7cbb91f8-f634-4aca-a148-384bc277c890"
   },
   "outputs": [],
   "source": [
    "!pip install transformers sacremoses datasets evaluate sentencepiece torch matplotlib scikit-learn protobuf==3.20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc4bdf-a4b6-4647-9cca-58251588c456",
   "metadata": {},
   "source": [
    "We're going to load the SQuAD data again now. If for some reason it's not available, we should download it again as during the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f1e2d-2055-4493-95f8-ec7cfab76df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('train-v2.0.json') as f:\n",
    "    train = json.load(f)[\"data\"]\n",
    "    \n",
    "with open('dev-v2.0.json') as f:\n",
    "    dev = json.load(f)[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ec110-1719-4329-ab18-e49f9b4d1dc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing data\n",
    "\n",
    "We'll need to extract the data that interests us from the SQuAD dataset. For this task we're interested in extracting all the question-answer pairs.\n",
    "Let's:\n",
    "1. Create a list of dictionaries containing 'text' and 'labels' keys.\n",
    "2. Each 'labels' entry should be an answer.\n",
    "3. Let's format each field as follows: `question: <question> context: <context>`.\n",
    "\n",
    "Here's an example of how one of our dictionaries could look after the transformation:\n",
    "```\n",
    "  {'text': 'question: What areas did Beyonce compete in when she was growing up? context: Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyonc√©\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
    "   'labels': 'singing and dancing'}],\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828acc13-dd91-4c47-9510-9a0ba1147ab9",
   "metadata": {},
   "source": [
    "#### ‚≠êÔ∏è Implement the extract_qa_pairs function based on the instructions above\n",
    "\n",
    "<details>\n",
    "    <summary>hint\n",
    "    </summary>\n",
    "    \n",
    "```python\n",
    "def extract_qa_pairs(dset):\n",
    "    for topic in dset:\n",
    "        for paragraph in topic[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                # Questions without answers are not helpful for us, so let's skip them:\n",
    "                if qa['is_impossible']:\n",
    "                    continue\n",
    "                    \n",
    "                answers = set(a[\"text\"] for a in qa.get(\"answers\", []))\n",
    "                for a in answers:\n",
    "                    yield {'text': f\"question: {qa['question']} context: {paragraph['context']}\", 'labels': a}\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df971d04-fc5e-41c7-9b98-0d84cb6ea2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qa_pairs(dset):\n",
    "    for topic in dset:\n",
    "        for paragraph in topic[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                # Questions without answers are not helpful for us, so let's skip them:\n",
    "                if qa['is_impossible']:\n",
    "                    continue\n",
    "                    \n",
    "                answers = set(a[\"text\"] for a in qa.get(\"answers\", []))\n",
    "                for a in answers:\n",
    "                    yield {'text': f\"question: {qa['question']} context: {paragraph['context']}\", 'labels': a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8750c-2def-4074-b776-8ca437cecee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qas = list(extract_qa_pairs(train))\n",
    "dev_qas = list(extract_qa_pairs(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd000dd-2642-4da7-924b-7af5743791d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_qas) # 86821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22711d3b-96d7-47fd-a4eb-c2b0744bb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_qas) # 10279"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb11fc2-c3a9-4c0a-b825-2b30c7b77ee3",
   "metadata": {},
   "source": [
    "It's always a good practice to take some samples from the data to make sure we didn't make any silly mistakes (such as mixing up question and context fields or duplicating samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84117f3b-2b6a-4753-8f9a-d479dde1854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qas[:2], dev_qas[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d48ceb-899e-491e-ab57-b120a6abe55f",
   "metadata": {},
   "source": [
    "We can now pack this data to a ü§ó dataset and store it on our hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee6df76-f65a-4d48-bc50-5ead3d27c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_qas_dset = Dataset.from_list(train_qas)\n",
    "dev_qas_dset = Dataset.from_list(dev_qas)\n",
    "datasets = DatasetDict({\"train\": train_qas_dset, \"dev\": dev_qas_dset})\n",
    "datasets.save_to_disk(\"question-answering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd4a82-cc09-4ee9-877e-07c396bc464a",
   "metadata": {},
   "source": [
    "Let's see how our transformed dataset looks now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3304f-6493-43ba-b7ce-589cd39d44ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets[\"train\"][0][\"text\"])\n",
    "print(datasets[\"train\"][0][\"labels\"])\n",
    "print(datasets[\"dev\"][0][\"text\"])\n",
    "print(datasets[\"dev\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014068ca-0511-4652-b117-1ff7ba322f97",
   "metadata": {},
   "source": [
    "## Blind Tokenization\n",
    "\n",
    "We're going to try to tokenize the inputs the same way as we did during the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c4b76-5373-496f-9d8c-3ae16dbc642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b775ca1-a7a3-4e2e-8277-fe9605937ba6",
   "metadata": {},
   "source": [
    "There's a couple of differences we need to apply this time, since we're solving a different task. Namely, instead of having binary labels (0 and 1 for possible/impossible questions) we're having text results now. Text results have to be tokenized same as text inputs. \n",
    "\n",
    "The flow is almost the same, we just need to use the text_target parameter when calling the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fc8ac-5e09-474a-9303-cf4248f4b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"])\n",
    "    labels = tokenizer(text_target=examples[\"labels\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6d98e-f746-4537-99c6-a035ed21a575",
   "metadata": {},
   "source": [
    "Let's tokenize the datasets with the preprocess function we've just implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ace0c-29ab-4b29-b392-0e46bbaf8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf2ca0-5b41-4d7c-993d-c636b674830d",
   "metadata": {},
   "source": [
    "You should have received the following error:\n",
    "> Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
    "\n",
    "It seems like some of our inputs are too long and could lead to issues while handling the data. Let's try to see a bit better what's happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc62dac-f0da-47aa-803c-65f29aeed95d",
   "metadata": {},
   "source": [
    "## Data-driven tokenization\n",
    "\n",
    "Let's try to draw distributions of our input lengths after tokenization.\n",
    "\n",
    "We're going to use matplotlib for plotting the histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9594c04-195d-485d-83cd-591c94cd36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a99cb0-babf-4175-8c27-b529cea56c82",
   "metadata": {},
   "source": [
    "Let's see how many tokens comprise each of our questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd002c-0de0-4c11-af99-207009e86d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_q_tokens = [len(x) for x in tokenized_datasets[\"train\"][\"input_ids\"]]\n",
    "len_a_tokens = [len(x) for x in tokenized_datasets[\"train\"][\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af861ae4-7510-475d-a26d-14e2ea738900",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(len_q_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d55a1-c9ba-4ac9-9990-d0fba945b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(len_a_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70106955-66a1-4ccc-97f4-06e9e201d213",
   "metadata": {},
   "source": [
    "We can see that indeed, there are some samples longer than 512. We should revisit our tokenization process and update it so that every question in our dataset will contain no more than 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4855d66-897c-4838-b6e0-e883d38c12f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    result = tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "    targets = tokenizer(\n",
    "        examples[\"labels\"], truncation=True, max_length=32, padding=\"max_length\",\n",
    "    )\n",
    "    input_ids = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in e]\n",
    "        for e in targets[\"input_ids\"]\n",
    "    ]\n",
    "    result[\"labels\"] = input_ids\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c7721-ec6b-4689-9b97-15199516274f",
   "metadata": {},
   "source": [
    "#### ‚≠êÔ∏è Apply the new preprocess_function\n",
    "\n",
    "<details>\n",
    "    <summary>hint\n",
    "    </summary>\n",
    "    \n",
    "```python\n",
    "tokenized_datasets = datasets.map(preprocess_function, batched=True)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b288358-3c92-4d8d-8f1a-868b8c5d9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf286ded-ad58-4ec4-a455-9cd30a66beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"][0].keys())\n",
    "print(tokenized_datasets[\"train\"][0][\"input_ids\"])\n",
    "print(tokenized_datasets[\"train\"][0][\"labels\"])\n",
    "print(len(tokenized_datasets[\"train\"][0][\"input_ids\"]))\n",
    "print(len(tokenized_datasets[\"train\"][0][\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0324b8-e65d-4db6-90ee-51712e7ed9df",
   "metadata": {},
   "source": [
    "#### ‚≠êÔ∏è Plot length distributions of tokenized questions and answers for the fixed dataset\n",
    "Questions to answer:\n",
    "1. Did we resolve the question length issue?\n",
    "2. Has anything else changed?\n",
    "\n",
    "<details>\n",
    "    <summary>hint\n",
    "    </summary>\n",
    "\n",
    "Histogram codes:\n",
    "    \n",
    "```python\n",
    "input_ids = [len(x) for x in tokenized_datasets[\"train\"][\"input_ids\"]]\n",
    "plt.hist(input_ids)\n",
    "\n",
    "labels = [len(x) for x in tokenized_datasets[\"train\"][\"labels\"]]\n",
    "plt.hist(labels)\n",
    "```\n",
    "    \n",
    "    \n",
    "<br/>    \n",
    "Regarding the questions:\n",
    "    \n",
    "1. Yes, the issue should've been fixed.\n",
    "2. We also padded all the answers so they are all of the same length (32).\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028dbf9-ebf8-4f54-b204-6465c263865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ‚≠êÔ∏è plot the question length distribution here\n",
    "input_ids = [len(x) for x in tokenized_datasets[\"train\"][\"input_ids\"]]\n",
    "plt.hist(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66386c0-b8ac-410b-a47c-135682b60892",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ‚≠êÔ∏è plot the answer length distribution here\n",
    "labels = [len(x) for x in tokenized_datasets[\"train\"][\"labels\"]]\n",
    "plt.hist(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73d2e2-b605-486e-8bd4-bbb1417fe532",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc980a83-64cf-4c50-9d01-d067c878e6a9",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that our data has been tokenized, we can go ahead and set up all of our training pieces.\n",
    "\n",
    "**About the model**\n",
    "\n",
    "\n",
    "The QA problem will require us to use a different pre-trained model and a slightly different data preparation (which you've already noticed above). We're going to use the T5 model, which was trained on a **span corruption** task, meaning it was tasked with recovering fragments of texts - during the pretraining it was given texts which were missing some fragments and the model's goal was to 'recover' (i.e. regenerate) them. It was then also pretrained on a couple of downstream NLP tasks such as translation and question answering.\n",
    "\n",
    "You can sit back and relax, there will be no more tasks until the training from now on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeca074-f095-4db1-85b0-0b8aaf529dd7",
   "metadata": {},
   "source": [
    "### Preparing the evaluation\n",
    "Our evaluation code will consist of a couple of steps. This is because alongside numbers (metrics), we would also want to observe the actual responses our model is giving to the questions. This is why our 'compute_metrics' function will consist of the following:\n",
    "1. Decoding a chosen answer from the model and printing it.\n",
    "2. Computing standard metrics (bleu + exact match).\n",
    "    - **note to self** - remember to talk about ^ during the labs\n",
    "3. Computing average answer length of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c825f-49d1-49da-be17-2be37399f526",
   "metadata": {},
   "source": [
    "#### ‚≠êÔ∏è Bonus task ‚≠êÔ∏è\n",
    "\n",
    "There actually will be one more task for you. It should be fun, though üôè\n",
    "I mentioned that in the first part of the `compute_metrics` we're going to decode a chosen question. Your task is to pick a question you'd like to see our model improve on during the training (hint: try to pick a tough one).\n",
    "\n",
    "You can see the questions with the following code:\n",
    "```python\n",
    "print(\"Question:\", dev_qas_dset[<your chosen id>][\"text\"]) \n",
    "```\n",
    "Once you're done, update the cell below with your id. The one I chose is 36."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6a31b-1044-43f8-a778-abd40393d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_DEV_QUESTION_ID = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac41e7-6f6a-4bf7-9e22-7b2f455c3313",
   "metadata": {},
   "source": [
    "Now let's implement the `compute_metrics` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd85d1-4355-4479-ad02-38bc98928246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "exact = evaluate.load(\"exact_match\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Step 1: Decode the chosen answer\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    print(\"Question:\", dev_qas_dset[CHOSEN_DEV_QUESTION_ID][\"text\"]) \n",
    "    print(\"Predicted:\", decoded_preds[CHOSEN_DEV_QUESTION_ID])\n",
    "    print(\"Expected:\", decoded_labels[CHOSEN_DEV_QUESTION_ID])\n",
    "\n",
    "    # Step 2: Compute bleu and exact match\n",
    "    result = {\n",
    "        \"bleu\": bleu.compute(predictions=decoded_preds, references=decoded_labels)[\"bleu\"],\n",
    "        **exact.compute(predictions=decoded_preds, references=decoded_labels),\n",
    "    }\n",
    "\n",
    "    # Step 3: Compute mean generated answer length\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cdd7ab-7e05-4cd9-9597-f388383aa862",
   "metadata": {},
   "source": [
    "### Training args\n",
    "\n",
    "Let's prepare the training args now.\n",
    "\n",
    "**Do not set FP16 to True**: this will likely cause the model to be unable to train. Some more details on this can be found in this thread: https://discuss.huggingface.co/t/t5-finetuning-tips/684/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa8a09-47f9-49db-8605-1dcc01406e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='output_qa',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,    \n",
    "    learning_rate=1e-04,\n",
    "    num_train_epochs=3, \n",
    "    logging_first_step=True,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=50,\n",
    "    save_strategy='epoch',\n",
    "    fp16=False, \n",
    "    optim='adafactor',\n",
    "    eval_accumulation_steps=4,\n",
    "    generation_max_length=32,\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ff883-406e-49df-8eb2-3b899543f919",
   "metadata": {},
   "source": [
    "The head for our model needs to be different this time as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1973a-f171-4c5e-bae5-422803755c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee18412-ff3e-4f07-bb0c-e43ca21b68d6",
   "metadata": {},
   "source": [
    "Let's build the final Trainer object now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79a77f-3ed2-4162-8dc1-6164344e2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['dev'].select(range(512)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b9550-0b16-4532-9c3f-7fd954399a74",
   "metadata": {},
   "source": [
    "### Let's go üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87610b9e-a2f2-4e58-971c-aebf87985406",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output/runs --host 0.0.0.0 --port=9002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765f496-f5fd-4107-9432-7b5daa83672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ae73c-44b7-425a-8cc5-81bdaa62f321",
   "metadata": {},
   "source": [
    "#### ‚≠êÔ∏è Bonus\n",
    "\n",
    "If you've trained the model, you can evaluate it and compare it with another model, e.g. https://huggingface.co/distilbert-base-cased-distilled-squad (which is extractive, in this case)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
